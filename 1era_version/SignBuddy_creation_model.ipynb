{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c711f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# Directorios para entrenamiento, validación y prueba\n",
    "train_dir = 'new_dataset/train' \n",
    "NP_PATH = 'new_dataset/NP_PATH'\n",
    "\n",
    "actions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c956d9-b6dd-4af5-b24a-0424302f513e",
   "metadata": {},
   "source": [
    "# creating data training folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d89d3e-ccea-4cd9-a51b-7ccb3185581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating folders for the landmarks for each frame of each video\n",
    "try:\n",
    "    for action in os.listdir(train_dir):\n",
    "        action_path = os.path.join(train_dir, action)\n",
    "        if os.path.isdir(action_path):\n",
    "            print(f\"Processing action: {action}\")\n",
    "            actions.append(action)\n",
    "    for action in actions:\n",
    "        action_path=os.path.join(NP_PATH, action)\n",
    "        os.makedirs(action_path, exist_ok=True)\n",
    "        print(f'creando:{action_path}')\n",
    "        for video in range(5):\n",
    "            video_path=os.path.join(action_path, str(video))\n",
    "            os.makedirs(video_path, exist_ok=True)\n",
    "            print(f'creando:{video_path}')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b574c4-5db2-4878-b63d-e605ce17113d",
   "metadata": {},
   "source": [
    "# Preprocesing data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daa346f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  # Modelo Holístico\n",
    "\n",
    "def draw_landmarks(image, results_holistic):\n",
    "    mp_drawing = mp.solutions.drawing_utils  # Utilidades de dibujo\n",
    "\n",
    "    # Configuración para líneas más delgadas\n",
    "    landmark_drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "    connection_drawing_spec = mp_drawing.DrawingSpec(thickness=1)\n",
    "\n",
    "    if results_holistic.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results_holistic.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=landmark_drawing_spec,\n",
    "            connection_drawing_spec=connection_drawing_spec)\n",
    "    \n",
    "    if results_holistic.left_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results_holistic.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=landmark_drawing_spec,\n",
    "            connection_drawing_spec=connection_drawing_spec)\n",
    "    \n",
    "    if results_holistic.right_hand_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results_holistic.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "            landmark_drawing_spec=landmark_drawing_spec,\n",
    "            connection_drawing_spec=connection_drawing_spec)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def extract_keypoints(results_holistic):\n",
    "    # Extracción de keypoints de la pose\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results_holistic.pose_landmarks.landmark]).flatten() if results_holistic.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    # Extracción de keypoints de la mano izquierda\n",
    "    lh = np.zeros(21*3)\n",
    "    if results_holistic.left_hand_landmarks:\n",
    "        lh = np.array([[res.x, res.y, res.z] for res in results_holistic.left_hand_landmarks.landmark]).flatten()\n",
    "    \n",
    "    # Extracción de keypoints de la mano derecha\n",
    "    rh = np.zeros(21*3)\n",
    "    if results_holistic.right_hand_landmarks:\n",
    "        rh = np.array([[res.x, res.y, res.z] for res in results_holistic.right_hand_landmarks.landmark]).flatten()\n",
    "    \n",
    "    return np.concatenate([pose, lh, rh])\n",
    "    \n",
    "def process_all_videos(root_path, sequence_length):\n",
    "    \"\"\"\n",
    "    Procesa todos los videos en los subdirectorios de la ruta raíz especificada.\n",
    "    \n",
    "    Args:\n",
    "    root_path (str): Ruta al directorio raíz que contiene las carpetas de videos.\n",
    "    sequence_length (int): Número de frames a procesar por video.\n",
    "    NP_PATH (str): Ruta donde se guardarán los archivos numpy con los keypoints.\n",
    "    \"\"\"\n",
    "    # Configuración de MediaPipe\n",
    "    actions = []\n",
    "\n",
    "    with mp_holistic.Holistic(\n",
    "        static_image_mode=False,\n",
    "        model_complexity=2,                 # 0 para más rápido, 2 para más preciso\n",
    "        smooth_landmarks=True,\n",
    "        min_detection_confidence=0.6,       # Aumentar el umbral para reducir falsos positivos\n",
    "        min_tracking_confidence=0.6) as holistic:\n",
    "        \n",
    "        for action in os.listdir(root_path):\n",
    "            action_path = os.path.join(root_path, action)\n",
    "            if os.path.isdir(action_path):\n",
    "                print(f\"Processing action: {action}\")\n",
    "                num_video =0\n",
    "                for video in os.listdir(action_path):\n",
    "                    if video.endswith(('.mp4', '.avi', '.mov')):\n",
    "                        video_path = os.path.join(action_path, video)\n",
    "                        process_video(video_path, holistic, action, num_video, sequence_length)\n",
    "                        num_video+=1\n",
    "        print(\"Processed actions:\", actions)\n",
    "\n",
    "def process_video(video_path, holistic, action, video, sequence_length):\n",
    "    \"\"\"\n",
    "    Procesa un único video y extrae los landmarks.\n",
    "    \n",
    "    Args:\n",
    "    video_path (str): Ruta al archivo de video a procesar.\n",
    "    hands (mediapipe.solutions.hands.Hands): Instancia de MediaPipe Hands.\n",
    "    pose (mediapipe.solutions.pose.Pose): Instancia de MediaPipe Pose.\n",
    "    action (str): Nombre de la acción (directorio padre del video).\n",
    "    video (str): Nombre del archivo de video.\n",
    "    sequence_length (int): Número de frames a procesar por video.\n",
    "    NP_PATH (str): Ruta donde se guardarán los archivos numpy con los keypoints.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Crear una ventana para mostrar el video\n",
    "    window_name = f\"Processing: {action} - {video}\"\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(window_name, 640, 720)\n",
    "\n",
    "    for frame_num in range(sequence_length):\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results_holistic = holistic.process(image_rgb)\n",
    "        \n",
    "        # Dibujar los landmarks en la imagen\n",
    "        image_with_landmarks = draw_landmarks(image, results_holistic)\n",
    "        \n",
    "        # Mostrar la imagen procesada\n",
    "        cv2.imshow(window_name, image_with_landmarks)\n",
    "        \n",
    "        if frame_num == 0:\n",
    "            cv2.waitKey(500)\n",
    "        else:\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        keypoints = extract_keypoints(results_holistic)\n",
    "        npy_path = os.path.join(NP_PATH, action, str(video), str(frame_num))\n",
    "        np.save(npy_path, keypoints)\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyWindow(window_name)\n",
    "    print(f\"Processed {frame_num + 1} frames from {video_path}\")\n",
    "    # Agregar una pausa de 2 segundos entre videos\n",
    "    time.sleep(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the function\n",
    "\n",
    "process_all_videos(train_dir, 34) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "596009b7-627a-4dd3-965f-e674bff00298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c682ddbc-14a9-4391-9f7b-a2ab46557c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = list(pd.Series(actions).unique())\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "with open('datos.json', 'w') as f:\n",
    "    json.dump(label_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678149d6-7d55-4dae-b8f8-34b3245c5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb5da7-3ba8-4c60-bef4-d395bc5aee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sequences, labels = [], []\n",
    "    for action in actions:\n",
    "        print(action)\n",
    "        for sequence in np.array(os.listdir(os.path.join(NP_PATH, action))).astype(int):\n",
    "            print(sequence)\n",
    "            window = []\n",
    "            for frame_num in range(34):\n",
    "                print(frame_num)\n",
    "                res = np.load(os.path.join(NP_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "                window.append(res)\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f9fc4-9e56-435e-9dd8-a40bbbc5e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341886a-a1d7-40fc-9944-b48b5cc0cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a92b43ef-6799-40d6-975c-8076a56e7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "val_x, y_val=X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48a5783f-57e5-4770-857e-5f7f5243ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72412cc8-074c-40ee-9dd5-b4291d28923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18881259-d8b1-4f7f-a5ce-fc597defda2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.regularizers import l2 \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c2709-38ee-481e-a2bd-e51ae5fe1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8a3c6dc-4ee2-4a09-a264-f380bfd98b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    # Crear una figura con dos subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "    # Graficar la pérdida\n",
    "    ax1.plot(history.history['loss'], label='Pérdida de entrenamiento')\n",
    "    ax1.plot(history.history['val_loss'], label='Pérdida de validación')\n",
    "    ax1.set_title('Pérdida del modelo')\n",
    "    ax1.set_ylabel('Pérdida')\n",
    "    ax1.set_xlabel('Época')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Graficar la precisión\n",
    "    ax2.plot(history.history['categorical_accuracy'], label='Precisión de entrenamiento')\n",
    "    ax2.plot(history.history['val_categorical_accuracy'], label='Precisión de validación')\n",
    "    ax2.set_title('Precisión del modelo')\n",
    "    ax2.set_ylabel('Precisión')\n",
    "    ax2.set_xlabel('Época')\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Ajustar el espacio entre subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Mostrar la figura\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a4982-0698-4fb2-8376-37cbf99d3639",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9e05015b-4de2-4e55-aee3-152b72f431f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Definir el callback de Early Stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Métrica a monitorear\n",
    "    patience=30,         # Número de épocas a esperar antes de detener si no hay mejora\n",
    "    restore_best_weights=True,  # Restaurar los mejores pesos encontrados\n",
    "    verbose=1            # Mostrar mensaje cuando se detenga el entrenamiento\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(34, 258)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(64, return_sequences=False, kernel_regularizer=l2(0.03)))\n",
    "model.add(BatchNormalization())  # Normalización para mejorar la convergencia\n",
    "model.add(Dropout(0.5))  # Regularización para evitar overfitting\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.03)))\n",
    "model.add(Dense(38, activation='softmax'))  # 38 clases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Input(shape=(34,258)))\n",
    "# model.add(LSTM(32, return_sequences=True, activation='relu', ))\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(LSTM(128, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "# model.add(Dense(38, activation='softmax'))\n",
    "# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d763dd-7f61-4254-a030-ce977728d53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo con Early Stopping\n",
    "history = model.fit(X_train, y_train, \n",
    "          epochs=200,\n",
    "          validation_split= 0.1,#(X_train, y_train),  # Usar 20% de los datos para validación\n",
    "          callbacks=[tb_callback, early_stopping]) #reduce_lr  \n",
    "\n",
    "# Evaluar en el conjunto de prueba\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Loss en test: {test_loss}, Accuracy en test: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df7dc58-e98f-476d-a973-d2443a7e2cbf",
   "metadata": {},
   "source": [
    "# Tensorboard plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185dab5f-f3e8-4156-97e6-5d98cb2c426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorboard import program\n",
    "import webbrowser\n",
    "\n",
    "# Inicia TensorBoard\n",
    "tb = program.TensorBoard()\n",
    "\n",
    "tb.configure(argv=[None, '--logdir', 'Logs/train'])\n",
    "url = tb.launch()\n",
    "print(f\"TensorBoard iniciado en {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381c30e-710d-42d0-a5fc-4ff1e4850879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fe9f3-1d21-4d87-81c2-eb462e1a5333",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(X_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4acab9-4f2f-49be-9077-11533980b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(results[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bf50b-513a-437c-82b9-55a1c585382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0bfbec54-7c68-42d9-a5f4-0969f23071e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lector_model(95acc-95val_acc-95test).keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4299be60-5ed6-4577-8e47-d5de88b94cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## deleting model from the buffer\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c8e89cad-3ab2-4214-bd4c-d1a7a19fc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "model=keras.models.load_model('lector_model(99acc-97val_acc).keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "94357d6a-31d8-4690-8d46-25fc0c2bb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41543043-ba6d-404d-b3db-5a59cc1713b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f85856c1-c5d0-49cd-9fac-1610b06434ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc5cbe-e193-463c-b806-a94e1fb9c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde1d1d-d410-4133-9b4a-6afe639f5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2d7aa477-e32f-4c24-8fb9-7029b154a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6f99f-4fe1-437c-af86-59510378a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "\n",
    "last_prediction_time = 0\n",
    "prediction_interval = 1\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_pose = mp.solutions.pose\n",
    "hands = mp_hands.Hands()\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# def draw_landmarks(image, results_hands, results_pose):\n",
    "#     mp_drawing = mp.solutions.drawing_utils\n",
    "#     mp_hands = mp.solutions.hands\n",
    "#     mp_pose = mp.solutions.pose\n",
    "#     if results_pose.pose_landmarks:\n",
    "#         mp_drawing.draw_landmarks(\n",
    "#             image, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "#     if results_hands.multi_hand_landmarks:\n",
    "#         for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "#     return image\n",
    "def draw_landmarks(image, results_hands, results_pose):\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_pose = mp.solutions.pose\n",
    "\n",
    "    # Configuración para líneas más delgadas\n",
    "    landmark_drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "    connection_drawing_spec = mp_drawing.DrawingSpec(thickness=1)\n",
    "\n",
    "    if results_pose.pose_landmarks:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image, results_pose.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            landmark_drawing_spec=landmark_drawing_spec,\n",
    "            connection_drawing_spec=connection_drawing_spec)\n",
    "    \n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        for hand_landmarks in results_hands.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                landmark_drawing_spec=landmark_drawing_spec,\n",
    "                connection_drawing_spec=connection_drawing_spec)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def extract_keypoints(results_hands, results_pose):\n",
    "    # # pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    # # if results_hands.multi_hand_landmarks:\n",
    "    # #     lh = np.array([[res.x, res.y, res.z] for res in results_hands.multi_hand_landmarks[0].landmark]).flatten()\n",
    "    # #     if len(results_hands.multi_hand_landmarks) > 1:\n",
    "    # #         rh = np.array([[res.x, res.y, res.z] for res in results_hands.multi_hand_landmarks[1].landmark]).flatten()\n",
    "    # #     else:\n",
    "    # #         rh = np.zeros(21*3)\n",
    "    # # else:\n",
    "    # #     lh = np.zeros(21*3)\n",
    "    # #     rh = np.zeros(21*3)\n",
    "    \n",
    "    # # return np.concatenate([pose, lh, rh])\n",
    "    # pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\n",
    "    # lh = np.array([[res.x, res.y, res.z] for res in results_hands.left_hand_landmarks.landmark]).flatten() if results_hands.left_hand_landmarks else np.zeros(21*3)\n",
    "    # rh = np.array([[res.x, res.y, res.z] for res in results_hands.right_hand_landmarks.landmark]).flatten() if results_hands.right_hand_landmarks else np.zeros(21*3)\n",
    "    # return np.concatenate([pose, lh, rh])\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results_pose.pose_landmarks.landmark]).flatten() if results_pose.pose_landmarks else np.zeros(33*4)\n",
    "    \n",
    "    lh = np.zeros(21*3)\n",
    "    rh = np.zeros(21*3)\n",
    "    \n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        for idx, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n",
    "            hand = np.array([[res.x, res.y, res.z] for res in hand_landmarks.landmark]).flatten()\n",
    "            if idx == 0:\n",
    "                lh = hand\n",
    "            elif idx == 1:\n",
    "                rh = hand\n",
    "    \n",
    "    return np.concatenate([pose, lh, rh])\n",
    "\n",
    "\n",
    "def prob_viz(res, actions, image, color):\n",
    "    output_image = image.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_image, (0, 60+num*40), (int(prob*100), 90+num*40), color, -1)\n",
    "        cv2.putText(output_image, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return output_image\n",
    "\n",
    "# Define un solo color para todas las acciones (por ejemplo, azul)\n",
    "action_color = (255, 0, 0)  # BGR format (Blue)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Captura de video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results_hands = hands.process(image_rgb)\n",
    "    results_pose = pose.process(image_rgb)\n",
    "    \n",
    "    # Dibujar los landmarks en la imagen\n",
    "    image_with_landmarks = draw_landmarks(image, results_hands, results_pose)\n",
    "    \n",
    "    # Verificar si se detecta una mano\n",
    "    if results_hands.multi_hand_landmarks:\n",
    "        # Extraer keypoints\n",
    "        keypoints = extract_keypoints(results_hands, results_pose)\n",
    "        \n",
    "        # Añadir keypoints a la secuencia y mantener solo los últimos 33\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-33:]\n",
    "        current_time = time.time()\n",
    "        if len(sequence) == 33 and current_time - last_prediction_time >= prediction_interval:\n",
    "            # Realizar predicción\n",
    "            input_data = np.expand_dims(sequence, axis=0)\n",
    "            res = model.predict(input_data)[0]\n",
    "            predicted_action = actions[np.argmax(res)]\n",
    "            print(predicted_action)\n",
    "            predictions.append(np.argmax(res))\n",
    "            sequence.clear()  # Limpiar la secuencia cuando no se detecta mano\n",
    "            \n",
    "            # Lógica de visualización\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
    "                if res[np.argmax(res)] > threshold:\n",
    "                    if len(sentence) > 0:\n",
    "                        if predicted_action != sentence[-1]:\n",
    "                            sentence.append(predicted_action)\n",
    "                    else:\n",
    "                        sentence.append(predicted_action)\n",
    "            \n",
    "            \n",
    "            # Visualizar probabilidades\n",
    "            image_with_landmarks = prob_viz(res, actions, image_with_landmarks, action_color)\n",
    "            last_prediction_time = current_time\n",
    "        # Mostrar la predicción en la imagen\n",
    "        cv2.rectangle(image_with_landmarks, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image_with_landmarks, ' '.join(sentence), (3,30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    else:\n",
    "        # No se detecta mano\n",
    "        sequence.clear()  # Limpiar la secuencia cuando no se detecta mano\n",
    "        cv2.putText(image_with_landmarks, 'No hand detected', (10, 30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Mostrar la imagen procesada\n",
    "    cv2.imshow('Real-time Prediction', image_with_landmarks)\n",
    "    \n",
    "    # Salir si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab31dfdd-e21d-4bc8-bebd-090e14d66dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711949c9-757d-4a60-8ada-c79a5b50df75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samsung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
